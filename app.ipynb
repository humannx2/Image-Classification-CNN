{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, MaxPooling2D, Flatten, Conv2D\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir=\"archive/train\"\n",
    "test_dir=\"archive/test\"\n",
    "\n",
    "# Function to load and preprocess images\n",
    "def load_and_preprocess_image(path, label):\n",
    "    image = tf.io.read_file(path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)  # Decode the image\n",
    "    image = tf.image.resize(image, [64, 64])         # Resize the image\n",
    "    image = tf.cast(image, tf.float32) / 255.0       # Normalize the image to [0, 1]\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 64, 64, 3) (32,)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# Define paths to your dataset\n",
    "train_dir=\"archive/train\"\n",
    "test_dir=\"archive/test\"\n",
    "\n",
    "# Function to load and preprocess images\n",
    "def load_and_preprocess_image(path, label):\n",
    "    image = tf.io.read_file(path)  # Read the image file\n",
    "    image = tf.image.decode_jpeg(image, channels=3)  # Decode the image to RGB\n",
    "    image = tf.image.resize(image, [64, 64])         # Resize the image\n",
    "    # Data augmentation\n",
    "    image = tf.image.random_flip_left_right(image)   # Random horizontal flip\n",
    "    image = tf.image.random_flip_up_down(image)      # Random vertical flip\n",
    "    image = tf.image.random_contrast(image, lower=0.2, upper=0.5)  # Random contrast\n",
    "    image = tf.cast(image, tf.float32) / 255.0 \n",
    "    return image, label\n",
    "\n",
    "# Function to create a dataset from directories\n",
    "def create_dataset(directory, batch_size=32):\n",
    "    # List image files and their corresponding labels (cat=0, dog=1)\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    \n",
    "    # Walk through the directories and collect paths and labels\n",
    "    for label, class_name in enumerate(os.listdir(directory)):  # 0 for cat, 1 for dog\n",
    "        class_dir = os.path.join(directory, class_name)\n",
    "        if os.path.isdir(class_dir):\n",
    "            for img_file in os.listdir(class_dir):\n",
    "                image_paths.append(os.path.join(class_dir, img_file))\n",
    "                labels.append(label)\n",
    "    \n",
    "    # Create a TensorFlow Dataset from the paths and labels\n",
    "    path_ds = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "    # Map the `load_and_preprocess_image` function to each image\n",
    "    image_ds = path_ds.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    # Shuffle, batch, and prefetch the dataset for better performance\n",
    "    dataset = image_ds.shuffle(1000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Create training and testing datasets\n",
    "train_dataset = create_dataset(train_dir, batch_size=32)\n",
    "test_dataset = create_dataset(test_dir, batch_size=32)\n",
    "\n",
    "# Check the dataset output\n",
    "for images, labels in train_dataset.take(1):\n",
    "    print(images.shape, labels.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Augmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(path, label):\n",
    "    image = tf.io.read_file(path)  # Read the image file\n",
    "    image = tf.image.decode_jpeg(image, channels=3)  # Decode the image to RGB\n",
    "    image = tf.image.resize(image, [64, 64])         # Resize the image\n",
    "    \n",
    "    # Data augmentation\n",
    "    image = tf.image.random_flip_left_right(image)   # Random horizontal flip\n",
    "    image = tf.image.random_flip_up_down(image)      # Random vertical flip\n",
    "    image = tf.image.random_contrast(image, lower=0.2, upper=0.5)  # Random contrast\n",
    "    \n",
    "    image = tf.cast(image, tf.float32) / 255.0  # Normalize the image to [0, 1]\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(64, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    # Conv2D(128, (3, 3), activation='relu'),\n",
    "    # MaxPooling2D(2, 2),\n",
    "    # Conv2D(64, (3, 3), activation='relu'),\n",
    "    # MaxPooling2D(2, 2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Output layer for binary classification (cat vs dog)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 205ms/step - accuracy: 0.4743 - loss: 0.9448 - val_accuracy: 0.5000 - val_loss: 0.6947\n",
      "Epoch 2/25\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 165ms/step - accuracy: 0.5051 - loss: 0.6934 - val_accuracy: 0.5000 - val_loss: 0.6946\n",
      "Epoch 3/25\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 170ms/step - accuracy: 0.4669 - loss: 0.6967 - val_accuracy: 0.4929 - val_loss: 0.6933\n",
      "Epoch 4/25\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 155ms/step - accuracy: 0.5203 - loss: 0.6901 - val_accuracy: 0.4929 - val_loss: 0.7013\n",
      "Epoch 5/25\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 144ms/step - accuracy: 0.5552 - loss: 0.6891 - val_accuracy: 0.5643 - val_loss: 0.6943\n",
      "Epoch 6/25\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 145ms/step - accuracy: 0.5401 - loss: 0.6727 - val_accuracy: 0.5143 - val_loss: 0.7248\n",
      "Epoch 7/25\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 147ms/step - accuracy: 0.5866 - loss: 0.6672 - val_accuracy: 0.5857 - val_loss: 0.6924\n",
      "Epoch 8/25\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 148ms/step - accuracy: 0.5850 - loss: 0.6653 - val_accuracy: 0.5071 - val_loss: 0.7999\n",
      "Epoch 9/25\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 152ms/step - accuracy: 0.5684 - loss: 0.6938 - val_accuracy: 0.5786 - val_loss: 0.6909\n",
      "Epoch 10/25\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 155ms/step - accuracy: 0.6022 - loss: 0.6698 - val_accuracy: 0.5214 - val_loss: 0.6892\n",
      "Epoch 11/25\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 151ms/step - accuracy: 0.5788 - loss: 0.6743 - val_accuracy: 0.5357 - val_loss: 0.6975\n",
      "Epoch 12/25\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 160ms/step - accuracy: 0.5849 - loss: 0.6541 - val_accuracy: 0.5429 - val_loss: 0.7117\n",
      "Epoch 13/25\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 148ms/step - accuracy: 0.6214 - loss: 0.6479 - val_accuracy: 0.5929 - val_loss: 0.6826\n",
      "Epoch 14/25\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 172ms/step - accuracy: 0.6330 - loss: 0.6431 - val_accuracy: 0.5643 - val_loss: 0.6983\n",
      "Epoch 15/25\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 167ms/step - accuracy: 0.6572 - loss: 0.6441 - val_accuracy: 0.5714 - val_loss: 0.6817\n",
      "Epoch 16/25\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 161ms/step - accuracy: 0.5867 - loss: 0.6712 - val_accuracy: 0.5500 - val_loss: 0.7020\n",
      "Epoch 17/25\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 160ms/step - accuracy: 0.6037 - loss: 0.6526 - val_accuracy: 0.5143 - val_loss: 0.7128\n",
      "Epoch 18/25\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 144ms/step - accuracy: 0.6520 - loss: 0.6561 - val_accuracy: 0.5071 - val_loss: 0.7699\n",
      "Epoch 19/25\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 145ms/step - accuracy: 0.5977 - loss: 0.6888 - val_accuracy: 0.5571 - val_loss: 0.7068\n",
      "Epoch 20/25\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 145ms/step - accuracy: 0.6611 - loss: 0.6239 - val_accuracy: 0.5571 - val_loss: 0.6982\n",
      "Epoch 21/25\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 142ms/step - accuracy: 0.6135 - loss: 0.6325 - val_accuracy: 0.5643 - val_loss: 0.6961\n",
      "Epoch 22/25\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 143ms/step - accuracy: 0.6310 - loss: 0.6368 - val_accuracy: 0.6143 - val_loss: 0.6717\n",
      "Epoch 23/25\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 146ms/step - accuracy: 0.6499 - loss: 0.6331 - val_accuracy: 0.5857 - val_loss: 0.6884\n",
      "Epoch 24/25\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 155ms/step - accuracy: 0.6683 - loss: 0.5951 - val_accuracy: 0.5500 - val_loss: 0.6804\n",
      "Epoch 25/25\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 143ms/step - accuracy: 0.6261 - loss: 0.6620 - val_accuracy: 0.5571 - val_loss: 0.7332\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x122c7362690>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_dataset,\n",
    "    epochs=25,\n",
    "    validation_data=test_dataset,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.5349 - loss: 0.7470\n",
      "Test Loss: 0.7319509387016296\n",
      "Test Accuracy: 55.000001192092896\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_acc * 100}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
